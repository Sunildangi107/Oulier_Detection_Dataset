# Oulier_Detection_Dataset
# Efficient Outlier Detection Approach (EODA)

This repository contains Python implementations of three interconnected algorithms designed for efficient outlier detection in datasets:

1. **Feature Selection Algorithm (Algorithm 1)**: Selects relevant features by comparing Z-scores of original and shadow features.
2. **KNN Search Algorithm for Clustering (Algorithm 2)**: Enhances clustering by identifying valid neighbors based on Euclidean distances.
3. **Efficient Outlier Detection Algorithm (EODA) (Algorithm 3)**: Combines clustering results with density and distance metrics to identify outliers.

---

## Algorithms Overview

### Algorithm 1: Feature Selection
This algorithm selects relevant features by iteratively comparing Z-scores of original features with shadow features generated by shuffling the original dataset. Features with Z-scores greater than the maximum Z-score of shadow features are retained.

**Steps:**
1. Generate shadow features by shuffling the original dataset.
2. Compute Z-scores for original and shadow features using feature importance from a Random Forest model.
3. Retain features with Z-scores exceeding the maximum shadow feature Z-score.
4. Repeat for a specified number of iterations.

### Algorithm 2: KNN Search for Clustering
This algorithm clusters data points by iteratively identifying K-nearest neighbors (KNN) based on Euclidean distances. It ensures valid clustering by applying thresholds on distances.

**Steps:**
1. Initialize K=3 and set a maximum threshold `thresh_k`.
2. Compute distances between data points using the KNN algorithm.
3. Validate neighbors and form clusters based on distance thresholds.
4. Repeat until all points are assigned to clusters.

### Algorithm 3: Efficient Outlier Detection Approach (EODA)
This algorithm detects outliers by combining density and distance metrics within clusters. Data points with high outlier scores (based on density and nearest distances) are flagged as potential outliers.

**Steps:**
1. Compute the density of each cluster.
2. Calculate the nearest distances for each data point within a cluster.
3. Compute outlier scores as the product of density and nearest distance.
4. Sort points by outlier scores and return the top N outliers.

---

## Installation
1. Clone this repository:
   ```bash
   git clone <repository_url>
   cd <repository_folder>
   ```
2. Install the required libraries:
   ```bash
   pip install pandas numpy scikit-learn
   ```

---

## Usage

### Example Workflow
1. **Load Dataset**:
   ```python
   import pandas as pd
   dataset = pd.read_csv("data.csv")
   target = dataset["target"]
   data = dataset.drop("target", axis=1)
   ```

2. **Feature Selection**:
   ```python
   from feature_selection import feature_selection
   selected_data = feature_selection(data, target, max_iterations=10)
   ```

3. **KNN Clustering**:
   ```python
   from knn_search import knn_search
   clusters = knn_search(selected_data, thresh_k=5)
   ```

4. **Outlier Detection**:
   ```python
   from eoda import detect_outliers
   outliers = detect_outliers(selected_data, clusters)
   print("Detected Outliers:", outliers)
   ```

---

## Example Output
For a given dataset, the script identifies clusters and flags potential outliers, e.g.,
```
Detected Outliers: [Point_1, Point_23, Point_45]
```

---

## Contributing
Feel free to open issues or submit pull requests for improvements or additional features.

---

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## References
- **Feature Selection**: Inspired by Boruta algorithm using Random Forest.
- **KNN Clustering**: Built on standard KNN methodologies with enhancements for clustering.
- **EODA**: Combines clustering and density-based outlier detection techniques for high accuracy.
